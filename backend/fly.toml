# fly.toml app configuration file generated for story-maker on 2024-04-29T22:40:10-05:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'story-maker'
primary_region = 'bog'

[build]

[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0
  processes = ['app']

[env]
  # The Llama Cloud API key.
  LLAMA_CLOUD_API_KEY=""

  ## OpenAI
  # The provider for the AI models to use.
  MODEL_PROVIDER="openai"
  # The name of LLM model to use.
  MODEL="gpt-3.5-turbo"
  # Name of the embedding model to use.
  EMBEDDING_MODEL="text-embedding-3-large"

  ## Ollama
  # The provider for the AI models to use.
  #MODEL_PROVIDER="ollama"
  # The name of LLM model to use.
  OLAMA_MODEL="phi3"
  # Name of the embedding model to use.
  OLLAMA_EMBEDDING_MODEL="nomic-embed-text"
  # Dimension of the embedding model to use.
  OLLAMA_EMBEDDING_DIM="1536"


  # Dimension of the embedding model to use.
  EMBEDDING_DIM="1536"

  # The OpenAI API key to use.
  OPENAI_API_KEY=""

  # The address to start the backend app.
  APP_HOST="0.0.0.0"

  # The port to start the backend app.
  APP_PORT="8080"

  # Temperature for sampling from the model.
  # LLM_TEMPERATURE=

  # Maximum number of tokens to generate.
  # LLM_MAX_TOKENS=

  # The number of similar embeddings to return when retrieving documents.
  TOP_K="3"

  # The MongoDB connection URI.
  MONGO_URI=""

  MONGODB_DATABASE=""

  MONGODB_VECTORS=""

  MONGODB_VECTOR_INDEX="vector_index"

[[vm]]
  memory = '1gb'
  cpu_kind = 'shared'
  cpus = 1
